{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file _concrete_data_week3.zip_ appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder _concrete_data_week3_ appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: _train_ and _valid_. And if you explore these folders, you will find that each contains two subfolders: _positive_ and _negative_. These are the same folders that we saw in the labs in the previous modules of this course, where _negative_ is the negative class and it represents the concrete images with no cracks and _positive_ is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the _negative_ and _positive_ folders. This may consume all of your memory and you may end up with a **50\\*** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1.  We are obviously dealing with two classes, so _num_classes_ is 2. \n",
    "2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3.  We will be training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "image_resize = 224\n",
    "batch_size_training = 300\n",
    "batch_size_validation = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to _preprocess_input_ which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the _flow_from_directory_ method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the _flow_from_directory_ method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument _include_top_ and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the _layers_ attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.ModuleWrapper at 0x7f842478b820>,\n",
       " <keras.layers.core.Dense at 0x7f8424c09fa0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.ModuleWrapper at 0x7f842478b820>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the _summary_ attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "module_wrapper (ModuleWrappe (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_53), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_53), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_conv/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_54), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_54), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_55), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_55), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_55), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_56), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_56), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_57), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_57), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_56), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_0_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_57), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block1_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_58), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_58), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_58), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_59), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_59), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_59), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_60), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_60), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_60), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block2_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_61), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_61), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_61), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_62), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_62), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_62), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_63), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_63), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_63), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2_block3_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_64), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_conv/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_64), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_64), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_65), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_65), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_65), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_66), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_66), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_67), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_conv/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_67), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_66), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_0_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_67), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block1_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_68), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_68), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_68), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_69), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_69), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_69), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_70), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_70), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_70), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block2_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_71), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_71), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_71), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_72), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_72), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_72), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_73), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_73), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_73), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block3_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_74), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_74), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_74), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_75), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_75), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_75), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_76), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_76), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_76), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv3_block4_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_77), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_conv/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_77), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_77), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_78), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_78), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_78), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_79), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_79), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_80), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_conv/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_80), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_79), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_0_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_80), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block1_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_81), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_81), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_81), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_82), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_82), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_82), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_83), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_83), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_83), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block2_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_84), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_84), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_84), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_85), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_85), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_85), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_86), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_86), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_86), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block3_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_87), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_87), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_87), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_88), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_88), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_88), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_89), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_89), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_89), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block4_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_90), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_90), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_90), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_91), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_91), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_91), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_92), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_92), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_92), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block5_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_93), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_93), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_93), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_94), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_94), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_94), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_95), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_95), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_95), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv4_block6_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_96), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_conv/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_96), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_96), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_97), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_97), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_97), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_98), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_98), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_99), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_conv/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_99), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_98), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_0_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_99), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block1_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_100), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_100), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_100), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_101), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_101), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_101), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_102), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_102), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_102), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block2_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_103), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_103), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_103), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_104), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_104), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_104), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_105), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_105), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_105), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv5_block3_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    pooling='avg')  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.pad_2 (TFOpLambda) (None, 230, 230, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_53 (TFOpLambd (None, 112, 112, 64) 0           tf.compat.v1.pad_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_53 (TFOpLambda)  (None, 112, 112, 64) 0           tf.nn.convolution_53[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 112, 112, 64 0           tf.nn.bias_add_53[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_49 (TFOpLambda)      (None, 112, 112, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.pad_3 (TFOpLambda) (None, 114, 114, 64) 0           tf.nn.relu_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool_1 (TFO (None, 56, 56, 64)   0           tf.compat.v1.pad_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_54 (TFOpLambd (None, 56, 56, 64)   0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_54 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_54[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_54[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_50 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_55 (TFOpLambd (None, 56, 56, 64)   0           tf.nn.relu_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_55 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_55[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_55[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_51 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_57 (TFOpLambd (None, 56, 56, 256)  0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_56 (TFOpLambd (None, 56, 56, 256)  0           tf.nn.relu_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_57 (TFOpLambda)  (None, 56, 56, 256)  0           tf.nn.convolution_57[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_56 (TFOpLambda)  (None, 56, 56, 256)  0           tf.nn.convolution_56[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 256) 0           tf.nn.bias_add_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 256) 0           tf.nn.bias_add_56[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (None, 56, 56, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_52 (TFOpLambda)      (None, 56, 56, 256)  0           tf.__operators__.add_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_58 (TFOpLambd (None, 56, 56, 64)   0           tf.nn.relu_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_58 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_58[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_53 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_59 (TFOpLambd (None, 56, 56, 64)   0           tf.nn.relu_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_59 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_59[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_59[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_54 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_60 (TFOpLambd (None, 56, 56, 256)  0           tf.nn.relu_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_60 (TFOpLambda)  (None, 56, 56, 256)  0           tf.nn.convolution_60[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 256) 0           tf.nn.bias_add_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (None, 56, 56, 256)  0           tf.nn.relu_52[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_55 (TFOpLambda)      (None, 56, 56, 256)  0           tf.__operators__.add_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_61 (TFOpLambd (None, 56, 56, 64)   0           tf.nn.relu_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_61 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_61[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_61[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_56 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_62 (TFOpLambd (None, 56, 56, 64)   0           tf.nn.relu_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_62 (TFOpLambda)  (None, 56, 56, 64)   0           tf.nn.convolution_62[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 64), 0           tf.nn.bias_add_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_57 (TFOpLambda)      (None, 56, 56, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_63 (TFOpLambd (None, 56, 56, 256)  0           tf.nn.relu_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_63 (TFOpLambda)  (None, 56, 56, 256)  0           tf.nn.convolution_63[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 56, 56, 256) 0           tf.nn.bias_add_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (None, 56, 56, 256)  0           tf.nn.relu_55[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_58 (TFOpLambda)      (None, 56, 56, 256)  0           tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_64 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_64 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_64[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_64[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_59 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_65 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_65 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_65[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_65[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_60 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_67 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_66 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_67 (TFOpLambda)  (None, 28, 28, 512)  0           tf.nn.convolution_67[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_66 (TFOpLambda)  (None, 28, 28, 512)  0           tf.nn.convolution_66[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.bias_add_67[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.bias_add_66[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_61 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_68 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_68 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_68[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_68[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_62 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_69 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_69 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_69[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_69[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_63 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_70 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_70 (TFOpLambda)  (None, 28, 28, 512)  0           tf.nn.convolution_70[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.bias_add_70[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 28, 28, 512)  0           tf.nn.relu_61[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_64 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_71 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_71 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_71[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_71[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_65 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_72 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_72 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_72[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_72[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_66 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_73 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_73 (TFOpLambda)  (None, 28, 28, 512)  0           tf.nn.convolution_73[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.bias_add_73[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_21 (TFOpLa (None, 28, 28, 512)  0           tf.nn.relu_64[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_67 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_74 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_74 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_74[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_74[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_68 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_75 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_75 (TFOpLambda)  (None, 28, 28, 128)  0           tf.nn.convolution_75[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.bias_add_75[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_69 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_76 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_76 (TFOpLambda)  (None, 28, 28, 512)  0           tf.nn.convolution_76[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.bias_add_76[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_22 (TFOpLa (None, 28, 28, 512)  0           tf.nn.relu_67[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_70 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_77 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_77 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_77[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_77[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_71 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_78 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_78 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_78[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_78[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_72 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_80 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_79 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_80 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_80[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_79 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_79[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_80[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_79[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_23 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_73 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_81 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_81 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_81[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_81[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_74 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_82 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_82 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_82[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_82[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_75 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_83 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_83 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_83[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_83[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_24 (TFOpLa (None, 14, 14, 1024) 0           tf.nn.relu_73[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_76 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_84 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_84 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_84[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_84[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_77 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_85 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_85 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_85[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_85[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_78 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_86 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_86 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_86[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_86[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_25 (TFOpLa (None, 14, 14, 1024) 0           tf.nn.relu_76[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_79 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_87 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_87 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_87[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_87[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_80 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_88 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_88 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_88[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_88[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_81 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_89 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_89 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_89[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_89[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_26 (TFOpLa (None, 14, 14, 1024) 0           tf.nn.relu_79[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_82 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_90 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_90 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_90[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_90[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_83 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_91 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_91 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_91[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_91[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_84 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_92 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_92 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_92[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_92[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_27 (TFOpLa (None, 14, 14, 1024) 0           tf.nn.relu_82[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_85 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_93 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_93 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_93[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_93[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_86 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_94 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_94 (TFOpLambda)  (None, 14, 14, 256)  0           tf.nn.convolution_94[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.bias_add_94[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_87 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_95 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_95 (TFOpLambda)  (None, 14, 14, 1024) 0           tf.nn.convolution_95[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.bias_add_95[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_28 (TFOpLa (None, 14, 14, 1024) 0           tf.nn.relu_85[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_88 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_28[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_96 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_96 (TFOpLambda)  (None, 7, 7, 512)    0           tf.nn.convolution_96[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_96[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_89 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_97 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_97 (TFOpLambda)  (None, 7, 7, 512)    0           tf.nn.convolution_97[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_97[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_90 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_99 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_98 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_99 (TFOpLambda)  (None, 7, 7, 2048)   0           tf.nn.convolution_99[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_98 (TFOpLambda)  (None, 7, 7, 2048)   0           tf.nn.convolution_98[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.bias_add_99[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.bias_add_98[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_29 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_91 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_100 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_100 (TFOpLambda) (None, 7, 7, 512)    0           tf.nn.convolution_100[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_100[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_92 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_101 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_101 (TFOpLambda) (None, 7, 7, 512)    0           tf.nn.convolution_101[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_101[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_93 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_102 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_102 (TFOpLambda) (None, 7, 7, 2048)   0           tf.nn.convolution_102[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.bias_add_102[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_30 (TFOpLa (None, 7, 7, 2048)   0           tf.nn.relu_91[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_94 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_30[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_103 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_103 (TFOpLambda) (None, 7, 7, 512)    0           tf.nn.convolution_103[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_103[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_95 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_104 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_104 (TFOpLambda) (None, 7, 7, 512)    0           tf.nn.convolution_104[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.bias_add_104[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_96 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_105 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.bias_add_105 (TFOpLambda) (None, 7, 7, 2048)   0           tf.nn.convolution_105[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.bias_add_105[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_31 (TFOpLa (None, 7, 7, 2048)   0           tf.nn.relu_94[0][0]              \n",
      "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_97 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_1 (TFOpLamb (None, 2048)         0           tf.nn.relu_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            4098        tf.math.reduce_mean_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,098\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 3224s 11s/step - loss: 0.0434 - accuracy: 0.9899 - val_loss: 0.0056 - val_accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Layer tf.nn.convolution_53 was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'strides': [<class 'int'>, <class 'int'>], 'padding': <class 'str'>, 'dilations': [<class 'int'>, <class 'int'>], 'data_format': <class 'str'>, 'name': <class 'str'>}. They cannot be serialized out when saving the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/node.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ds/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ds/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ds/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/saved_model/json_utils.py\u001b[0m in \u001b[0;36mget_json_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ('Not JSON Serializable:', <tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\narray([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n          -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n         [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n           1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n         [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n          -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n\n        [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n          -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n         [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n           1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n         [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n           2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n\n        [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n          -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n         [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n           1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n         [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n          -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n\n        ...,\n\n        [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n          -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n         [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n           1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n         [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n           9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n\n        [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n          -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n         [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n           1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n         [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n          -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n\n        [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n          -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n         [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n           5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n         [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n          -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n\n\n       [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n          -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n         [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n           1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n         [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n          -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n\n        [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n          -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n         [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n           1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n         [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n          -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n\n        [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n           1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n         [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n           1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n         [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n          -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n\n        ...,\n\n        [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n          -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n         [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n           8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n         [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n          -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n\n        [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n          -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n         [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n           1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n         [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n          -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n\n        [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n          -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n         [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n           1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n         [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n          -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n\n\n       [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n          -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n         [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n           1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n         [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n          -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n\n        [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n          -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n         [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n           2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n         [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n          -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n\n        [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n           3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n         [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n           5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n         [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n           2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n\n        ...,\n\n        [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n           4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n         [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n           1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n         [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n          -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n\n        [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n          -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n         [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n           1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n         [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n          -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n\n        [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n          -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n         [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n           1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n         [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n          -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n\n\n       ...,\n\n\n       [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n          -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n         [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n           1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n         [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n          -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n\n        [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n          -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n         [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n           7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n         [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n          -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n\n        [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n           1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n         [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n           2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n         [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n          -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n\n        ...,\n\n        [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n          -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n         [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n           1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n         [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n          -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n\n        [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n          -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n         [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n           7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n         [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n          -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n\n        [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n          -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n         [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n           1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n         [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n          -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n\n\n       [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n          -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n         [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n           1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n         [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n          -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n\n        [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n          -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n         [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n           7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n         [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n          -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n\n        [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n          -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n         [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n           7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n         [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n          -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n\n        ...,\n\n        [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n          -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n         [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n           8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n         [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n          -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n\n        [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n          -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n         [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n           8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n         [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n          -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n\n        [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n          -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n         [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n           8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n         [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n          -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n\n\n       [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n          -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n         [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n           1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n         [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n          -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n\n        [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n          -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n         [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n           1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n         [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n          -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n\n        [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n          -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n         [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n           1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n         [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n          -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n\n        ...,\n\n        [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n          -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n         [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n           1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n         [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n          -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n\n        [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n          -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n         [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n           1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n         [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n          -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n\n        [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n          -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n         [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n           9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n         [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n          -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n      dtype=float32)>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0972fb0915b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classifier_resnet_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2084\u001b[0m     \"\"\"\n\u001b[1;32m   2085\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   2087\u001b[0m                     signatures, options, save_traces)\n\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m--> 146\u001b[0;31m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[1;32m    147\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   1345\u001b[0m           \u001b[0;31m# The node is relevant to the model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;31m# add to filtered_inbound_nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m           \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_node_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_conversion_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/node.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0mkwarg_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       raise TypeError('Layer ' + self.layer.name +\n\u001b[0m\u001b[1;32m    190\u001b[0m                       \u001b[0;34m' was passed non-JSON-serializable arguments. '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                       \u001b[0;34m'Arguments had types: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Layer tf.nn.convolution_53 was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'strides': [<class 'int'>, <class 'int'>], 'padding': <class 'str'>, 'dilations': [<class 'int'>, <class 'int'>], 'data_format': <class 'str'>, 'name': <class 'str'>}. They cannot be serialized out when saving the model."
     ]
    }
   ],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file _classifier_resnet_model.h5_ apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called _AI Capstone Project with Deep Learning_. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright © 2020 [IBM Developer Skills Network](https://cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork-20647850&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
